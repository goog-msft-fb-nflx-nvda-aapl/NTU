{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Curriculum Number: IE5054\n",
    "* Course Name: Data Analytics\n",
    "* Student ID: r13921031\n",
    "* Subject: Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Question 1. \n",
    "Given a simple linear regression model\n",
    "$ 𝑦_𝑖 = 𝛽_0 + 𝛽_1 * 𝑥_𝑖 + 𝜖_𝑖 $  \n",
    "$ 𝑖 = 1, … , 𝑛$  \n",
    "$ 𝜖_𝑖 $ is i.i.d. to $ N(𝜇, 𝜎^2).$  \n",
    "Prove that:  \n",
    "### ✅ 1-a. $cov(\\hat{𝛽_0},\\hat{𝛽_1} ) = − \\frac{\\overline{x} * 𝜎^2 }{ 𝑆_{𝑥𝑥} }$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 参考\n",
    " - [Mathematics][1] (https://math.stackexchange.com/questions/788574/find-operatornamecov-hat-beta-0-hat-beta-1)\n",
    " - [Mathematics][2] (https://math.stackexchange.com/questions/821318/operatornamevar-hat-beta-1-frac-sigma2-sumx-i-barx2-how-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ① 第一歩 \n",
    "We just use the result on page 30 of \"DA02 Regression Analysis.pdf\"\n",
    "$Var[\\hat{\\beta}_1]=\\frac{\\sigma^2}{S_{xx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ② 第二歩 \n",
    "$Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = Cov(\\bar{y} - \\hat{\\beta}_1 \\bar{x}, \\hat{\\beta}_1)$\n",
    "\n",
    "$= E[(\\bar{y} - \\hat{\\beta}_1 \\bar{x})\\hat{\\beta}_1] - E[\\bar{y} - \\hat{\\beta}_1 \\bar{x}]E[\\hat{\\beta}_1]$\n",
    "\n",
    "$= E[\\bar{y}\\hat{\\beta}_1 - \\bar{x}\\hat{\\beta}_1^2] - (\\bar{y} - \\bar{x}\\beta_1)\\beta_1$\n",
    "\n",
    "$= \\bar{y}E[\\hat{\\beta}_1] - \\bar{x}E[\\hat{\\beta}_1^2] - \\bar{y}\\beta_1 + \\bar{x}E[\\hat{\\beta}_1]E[\\hat{\\beta}_1]$\n",
    "\n",
    "$= \\bar{y}\\beta_1 - \\bar{x}E[\\hat{\\beta}_1^2] - \\bar{y}\\beta_1 + \\bar{x}\\beta_1^2$\n",
    "\n",
    "$= -\\bar{x}(E[\\hat{\\beta}_1^2] - E[\\hat{\\beta}_1]E[\\hat{\\beta}_1])$\n",
    "\n",
    "$= -\\bar{x}Var[\\hat{\\beta}_1]$\n",
    "$= -\\bar{x}\\frac{\\sigma^2}{S_{xx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👍 DONE.\n",
    "## ⚠️ The following part is just a stpe-by-step derviation for myself, can just ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}$\n",
    "\n",
    "$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$\n",
    "\n",
    "Where:\n",
    "- $S_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2$\n",
    "- $S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "Step 1: Express $\\hat{\\beta}_0$ in terms of $y_i$\n",
    "$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{\\beta}_1 \\bar{x}$\n",
    "\n",
    "Step 2: Calculate the covariance\n",
    "$Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = E[(\\hat{\\beta}_0 - E[\\hat{\\beta}_0])(\\hat{\\beta}_1 - E[\\hat{\\beta}_1])]$\n",
    "\n",
    "Step 3: Since both estimators are unbiased, we have:\n",
    "$E[\\hat{\\beta}_0] = \\beta_0$ and $E[\\hat{\\beta}_1] = \\beta_1$\n",
    "\n",
    "Therefore:\n",
    "$Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = E[(\\hat{\\beta}_0 - \\beta_0)(\\hat{\\beta}_1 - \\beta_1)]$\n",
    "\n",
    "Step 4: Substitute the expression for $\\hat{\\beta}_0$\n",
    "$\\hat{\\beta}_0 - \\beta_0 = \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{\\beta}_1 \\bar{x} - \\beta_0$\n",
    "\n",
    "Since $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$, we get:\n",
    "$\\hat{\\beta}_0 - \\beta_0 = \\frac{1}{n}\\sum_{i=1}^{n} (\\beta_0 + \\beta_1 x_i + \\epsilon_i) - \\hat{\\beta}_1 \\bar{x} - \\beta_0$\n",
    "$= \\beta_0 + \\beta_1 \\bar{x} + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i - \\hat{\\beta}_1 \\bar{x} - \\beta_0$\n",
    "$= \\beta_1 \\bar{x} + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i - \\hat{\\beta}_1 \\bar{x}$\n",
    "$= (\\beta_1 - \\hat{\\beta}_1)\\bar{x} + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i$\n",
    "$= -(\\hat{\\beta}_1 - \\beta_1)\\bar{x} + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i$\n",
    "\n",
    "Step 5: Recall the formula for $\\hat{\\beta}_1$\n",
    "$\\hat{\\beta}_1 - \\beta_1 = \\frac{S_{xy}}{S_{xx}} - \\beta_1$\n",
    "\n",
    "Substituting the model equation:\n",
    "$S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) = \\sum_{i=1}^{n} (x_i - \\bar{x})(\\beta_0 + \\beta_1 x_i + \\epsilon_i - \\bar{y})$\n",
    "\n",
    "Since $\\bar{y} = \\beta_0 + \\beta_1 \\bar{x} + \\bar{\\epsilon}$, we get:\n",
    "$S_{xy} = \\sum_{i=1}^{n} (x_i - \\bar{x})(\\beta_1(x_i - \\bar{x}) + (\\epsilon_i - \\bar{\\epsilon}))$\n",
    "$= \\beta_1 \\sum_{i=1}^{n} (x_i - \\bar{x})^2 + \\sum_{i=1}^{n} (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon})$\n",
    "$= \\beta_1 S_{xx} + \\sum_{i=1}^{n} (x_i - \\bar{x})\\epsilon_i$ (since $\\sum_{i=1}^{n} (x_i - \\bar{x}) = 0$)\n",
    "\n",
    "Therefore:\n",
    "$\\hat{\\beta}_1 - \\beta_1 = \\frac{\\beta_1 S_{xx} + \\sum_{i=1}^{n} (x_i - \\bar{x})\\epsilon_i}{S_{xx}} - \\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})\\epsilon_i}{S_{xx}}$\n",
    "\n",
    "Step 6: Now we can compute the covariance\n",
    "$Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = E[(\\hat{\\beta}_0 - \\beta_0)(\\hat{\\beta}_1 - \\beta_1)]$\n",
    "$= E[(-(\\hat{\\beta}_1 - \\beta_1)\\bar{x} + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i)(\\frac{\\sum_{j=1}^{n} (x_j - \\bar{x})\\epsilon_j}{S_{xx}})]$\n",
    "$= E[(-\\bar{x})(\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})\\epsilon_i}{S_{xx}})(\\frac{\\sum_{j=1}^{n} (x_j - \\bar{x})\\epsilon_j}{S_{xx}}) + (\\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i)(\\frac{\\sum_{j=1}^{n} (x_j - \\bar{x})\\epsilon_j}{S_{xx}})]$\n",
    "\n",
    "Step 7: Since $\\epsilon_i$ are i.i.d. with variance $\\sigma^2$, we have:\n",
    "$E[\\epsilon_i \\epsilon_j] = \\begin{cases} \\sigma^2, & \\text{if } i = j \\\\ 0, & \\text{if } i \\neq j \\end{cases}$\n",
    "\n",
    "Simplifying the first term:\n",
    "$E[(\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})\\epsilon_i}{S_{xx}})^2] = \\frac{E[\\sum_{i=1}^{n}\\sum_{j=1}^{n}(x_i - \\bar{x})(x_j - \\bar{x})\\epsilon_i\\epsilon_j]}{S_{xx}^2}$\n",
    "$= \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\sigma^2}{S_{xx}^2} = \\frac{\\sigma^2 S_{xx}}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}}$\n",
    "\n",
    "The first term becomes: $-\\bar{x} \\cdot \\frac{\\sigma^2}{S_{xx}}$\n",
    "\n",
    "For the second term:\n",
    "$E[(\\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_i)(\\frac{\\sum_{j=1}^{n} (x_j - \\bar{x})\\epsilon_j}{S_{xx}})] = \\frac{1}{n \\cdot S_{xx}}E[\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\epsilon_i(x_j - \\bar{x})\\epsilon_j]$\n",
    "$= \\frac{1}{n \\cdot S_{xx}}\\sum_{i=1}^{n}(x_i - \\bar{x})E[\\epsilon_i^2] = \\frac{1}{n \\cdot S_{xx}}\\sum_{i=1}^{n}(x_i - \\bar{x})\\sigma^2$\n",
    "$= \\frac{\\sigma^2}{n \\cdot S_{xx}}\\sum_{i=1}^{n}(x_i - \\bar{x}) = 0$ (since $\\sum_{i=1}^{n}(x_i - \\bar{x}) = 0$)\n",
    "\n",
    "Step 8: Combining the terms, we get:\n",
    "$Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\bar{x} \\cdot \\frac{\\sigma^2}{S_{xx}} + 0 = -\\frac{\\bar{x} \\cdot \\sigma^2}{S_{xx}}$\n",
    "\n",
    "Therefore: $Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\frac{\\bar{x} \\cdot \\sigma^2}{S_{xx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ 1-b. $cov(\\bar{y}, \\hat{𝛽_1}) = 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving $ \\text{cov}(\\bar{y}, \\hat{\\beta}_1) = 0 $\n",
    "\n",
    "We’re given the simple linear regression model:\n",
    "\n",
    "$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i = 1, \\dots, n\n",
    "$\n",
    "\n",
    "where $ \\epsilon_i $ are i.i.d. $ N(\\mu, \\sigma^2) $. We also know:\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n",
    "$\n",
    "\n",
    "and some key variance/covariance properties:\n",
    "\n",
    "$\n",
    "\\text{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{S_{xx}}, \\quad \\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\frac{\\bar{x} \\sigma^2}{S_{xx}}\n",
    "$\n",
    "\n",
    "## Step 1: Express $ \\bar{y} $ and $ \\hat{\\beta}_1 $\n",
    "\n",
    "By definition:\n",
    "\n",
    "$\n",
    "\\bar{y} = \\frac{1}{n} \\sum y_i\n",
    "$\n",
    "\n",
    "Expanding $ y_i $:\n",
    "\n",
    "$\n",
    "\\bar{y} = \\frac{1}{n} \\sum (\\beta_0 + \\beta_1 x_i + \\epsilon_i)\n",
    "$\n",
    "\n",
    "$\n",
    "= \\beta_0 + \\beta_1 \\bar{x} + \\bar{\\epsilon}\n",
    "$\n",
    "\n",
    "where $ \\bar{\\epsilon} = \\frac{1}{n} \\sum \\epsilon_i $.\n",
    "\n",
    "For $ \\hat{\\beta}_1 $, substituting $ y_i $ into $ S_{xy} $:\n",
    "\n",
    "$\n",
    "S_{xy} = \\sum (x_i - \\bar{x}) (\\beta_0 + \\beta_1 x_i + \\epsilon_i - \\bar{y})\n",
    "$\n",
    "\n",
    "$\n",
    "= \\sum (x_i - \\bar{x}) (\\beta_1 (x_i - \\bar{x}) + \\epsilon_i - \\bar{\\epsilon})\n",
    "$\n",
    "\n",
    "which simplifies to:\n",
    "\n",
    "$\n",
    "S_{xy} = \\beta_1 S_{xx} + \\sum (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon})\n",
    "$\n",
    "\n",
    "So:\n",
    "\n",
    "$\n",
    "\\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon})}{S_{xx}}\n",
    "$\n",
    "\n",
    "## Step 2: Compute $ \\text{cov}(\\bar{y}, \\hat{\\beta}_1) $\n",
    "\n",
    "By definition:\n",
    "\n",
    "$\n",
    "\\text{cov}(\\bar{y}, \\hat{\\beta}_1) = E[(\\bar{y} - E[\\bar{y}])(\\hat{\\beta}_1 - E[\\hat{\\beta}_1])]\n",
    "$\n",
    "\n",
    "We already found:\n",
    "\n",
    "$\n",
    "\\bar{y} - E[\\bar{y}] = \\bar{\\epsilon}, \\quad \\hat{\\beta}_1 - E[\\hat{\\beta}_1] = \\frac{\\sum (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon})}{S_{xx}}\n",
    "$\n",
    "\n",
    "So:\n",
    "\n",
    "$\n",
    "\\text{cov}(\\bar{y}, \\hat{\\beta}_1) = E \\left[ \\bar{\\epsilon} \\cdot \\frac{\\sum (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon})}{S_{xx}} \\right]\n",
    "$\n",
    "\n",
    "Since $ \\bar{\\epsilon} $ is independent of $ \\sum (x_i - \\bar{x})(\\epsilon_i - \\bar{\\epsilon}) $ and both have an expected value of 0, we get:\n",
    "\n",
    "$\n",
    "\\text{cov}(\\bar{y}, \\hat{\\beta}_1) = 0\n",
    "$\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "We’ve just shown that $ \\bar{y} $ (the sample mean of $ y $) and $ \\hat{\\beta}_1 $ (the estimated slope) are uncorrelated:\n",
    "\n",
    "$\n",
    "\\text{cov}(\\bar{y}, \\hat{\\beta}_1) = 0\n",
    "$\n",
    "\n",
    "That’s it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Question 2. (10%) \n",
    "Demonstrate that the regression sum of squares (SSR) can be computed using the following formula: $$ {SSR} = (\\sum_{i=1,2,...n}{(\\hat{y_i})^2}) - n * (\\bar{y})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 参考\n",
    " - [Datacamp SSR][1] (https://www.datacamp.com/tutorial/regression-sum-of-squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚨 Page 28 of DA02 Regression Analysis.pdf\n",
    "In linear regression, one LSE property is that $\\sum_{i=1}^{n}{\\hat{y}_i} = \\sum_{i=1}^{n}{y_i}$\n",
    "\n",
    "This implies that $\\sum_{i=1}^{n}{\\hat{y}_i} = n\\bar{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SSR = \\sum_{i=1}^{n}{(\\hat{y}_i - \\bar{y})^2}$\n",
    "\n",
    "- $\\hat{y}_i$ is the predicted value for observation $i$\n",
    "- $\\bar{y}$ is the mean of the observed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SSR = \\sum_{i=1}^{n}{(\\hat{y}_i - \\bar{y})^2}$\n",
    "\n",
    "$= \\sum_{i=1}^{n}{(\\hat{y}_i^2 - 2\\hat{y}_i\\bar{y} + \\bar{y}^2)}$\n",
    "\n",
    "$= \\sum_{i=1}^{n}{\\hat{y}_i^2} - \\sum_{i=1}^{n}{2\\hat{y}_i\\bar{y}} + \\sum_{i=1}^{n}{\\bar{y}^2}$\n",
    "\n",
    "$= \\sum_{i=1}^{n}{\\hat{y}_i^2} - 2\\bar{y}\\sum_{i=1}^{n}{\\hat{y}_i} + \\sum_{i=1}^{n}{\\bar{y}^2}$\n",
    "\n",
    "$= \\sum_{i=1}^{n}{\\hat{y}_i^2} - 2\\bar{y}\\sum_{i=1}^{n}{\\hat{y}_i} + \\bar{y}^2\\sum_{i=1}^{n}{1}$\n",
    "\n",
    "$= \\sum_{i=1}^{n}{\\hat{y}_i^2} - 2\\bar{y}\\sum_{i=1}^{n}{\\hat{y}_i} + n\\bar{y}^2$  \n",
    "by 🚨, we get   \n",
    "$= \\sum_{i=1}^{n}{\\hat{y}_i^2} - n\\bar{y}^2$\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Question 3. (10%)   \n",
    "Given a multiple regression model: 𝐲 = 𝐗 * 𝛃 + 𝛜. Prove that the Least Squares Error (LSE) can be also expressed as\n",
    "$\\hat{\\beta} = 𝛃 + R * 𝛜$  \n",
    "where $𝐑 = ((𝐗^𝑇)*𝐗)^{−1}*(𝐗^𝑇)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression model:\n",
    "$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "\n",
    "The LSE (least squares estimator) $\\hat{\\boldsymbol{\\beta}}$ minimizes the sum of squared residuals:\n",
    "$\\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$\n",
    "\n",
    "Taking the derivative with respect to $\\boldsymbol{\\beta}$ and setting it to zero:\n",
    "$-2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$  \n",
    "\n",
    "$\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}$  \n",
    "\n",
    "$\\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{0}$  \n",
    "\n",
    "$\\mathbf{X}^T \\mathbf{y} = \\mathbf{X}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$  \n",
    "\n",
    "Assuming $\\mathbf{X}^T\\mathbf{X}$ is invertible. $\\Rightarrow \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "\n",
    "Subsituting multiple regression model $ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$ into this expression\n",
    "$\\Rightarrow \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})$\n",
    "\n",
    "$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}$\n",
    "\n",
    "$ = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\boldsymbol{\\epsilon}$\n",
    "\n",
    "$ = \\boldsymbol{\\beta} + \\mathbf{R}\\boldsymbol{\\epsilon}$\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Question 4. (10%) \n",
    "The matrix, $𝐗*(((𝐗^𝑇)*𝐗)^{−1})*(𝐗^𝑇)$, derived in multiple regression is usually defined as 𝐇 and called the hat matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ 4-a. 𝐇 is idempotent, i.e., 𝐇*𝐇 = 𝐇 and (𝐈 − 𝐇)(𝐈 − 𝐇) = (𝐈 − 𝐇) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{H} = \\mathbf{X}((\\mathbf{X}^T\\mathbf{X})^{-1})\\mathbf{X}^T$\n",
    "\n",
    "#### ✅ 4-a-a: Proving $\\mathbf{H}$ is idempotent, i.e., $\\mathbf{H}\\mathbf{H} = \\mathbf{H}$\n",
    "\n",
    "$\\mathbf{H}\\mathbf{H} = \\mathbf{X}((\\mathbf{X}^T\\mathbf{X})^{-1})\\mathbf{X}^T \\cdot \\mathbf{X}((\\mathbf{X}^T\\mathbf{X})^{-1})\\mathbf{X}^T$\n",
    "\n",
    "$\\mathbf{H}\\mathbf{H} = \\mathbf{X}((\\mathbf{X}^T\\mathbf{X})^{-1}) \\cdot \\mathbf{X}^T\\mathbf{X} \\cdot ((\\mathbf{X}^T\\mathbf{X})^{-1})\\mathbf{X}^T$\n",
    "\n",
    "$\\mathbf{H}\\mathbf{H} = \\mathbf{X}((\\mathbf{X}^T\\mathbf{X})^{-1}) \\cdot \\mathbf{X}^T$\n",
    "\n",
    "$\\mathbf{H}\\mathbf{H} = \\mathbf{H}$\n",
    "\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ 4-a-b: Proving $(\\mathbf{I} - \\mathbf{H})(\\mathbf{I} - \\mathbf{H}) = (\\mathbf{I} - \\mathbf{H})$\n",
    "\n",
    "$(\\mathbf{I} - \\mathbf{H})(\\mathbf{I} - \\mathbf{H}) = \\mathbf{I} - \\mathbf{H} - \\mathbf{H} + \\mathbf{H}\\mathbf{H}$\n",
    "\n",
    "$(\\mathbf{I} - \\mathbf{H})(\\mathbf{I} - \\mathbf{H}) = \\mathbf{I} - \\mathbf{H} - \\mathbf{H} + \\mathbf{H}$\n",
    "\n",
    "$(\\mathbf{I} - \\mathbf{H})(\\mathbf{I} - \\mathbf{H}) = \\mathbf{I} - \\mathbf{H}$\n",
    "\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✅ 4-b. $V(\\hat{𝐲})= (𝜎^2) * 𝐇$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple regression, we have:\n",
    "- $\\mathbf{y}$ is the vector of observed responses\n",
    "- $\\hat{\\mathbf{y}}$ is the vector of predicted values\n",
    "- $\\mathbf{X}$ is the design matrix\n",
    "- $\\boldsymbol{\\beta}$ is the vector of regression coefficients\n",
    "- $\\boldsymbol{\\varepsilon}$ is the vector of errors\n",
    "- $\\sigma^2$ is the variance of the error terms\n",
    "\n",
    "linear regression model:\n",
    "$\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$\n",
    "\n",
    "The ordinary least squares (OLS) estimator for $\\boldsymbol{\\beta}$ is:\n",
    "$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "\n",
    "The predicted values $\\hat{\\mathbf{y}}$ are given by:\n",
    "$\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$\n",
    "\n",
    "$\\Rightarrow \\hat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = \\mathbf{H}\\mathbf{y}$\n",
    "\n",
    "\n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = V(\\mathbf{H}\\mathbf{y})$  \n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = \\mathbf{H} \\cdot V(\\mathbf{y}) \\cdot \\mathbf{H}^T$\n",
    "\n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = \\mathbf{H} \\cdot (V(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon})) \\cdot \\mathbf{H}^T$\n",
    "\n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = \\mathbf{H} \\cdot (V(\\boldsymbol{\\varepsilon})) \\cdot \\mathbf{H}^T$\n",
    "\n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = \\mathbf{H} \\cdot (\\sigma^2\\mathbf{I}) \\cdot \\mathbf{H}^T$\n",
    "\n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = \\mathbf{H} \\cdot \\sigma^2\\mathbf{I} \\cdot \\mathbf{H}^T$\n",
    "\n",
    "$\\Rightarrow V(\\hat{\\mathbf{y}}) = \\sigma^2 \\cdot \\mathbf{H} \\cdot \\mathbf{H}^T$\n",
    "\n",
    "Since the hat matrix $\\mathbf{H}$ is symmetric, $\\mathbf{H}^T = \\mathbf{H}$.\n",
    "\n",
    "Therefore:\n",
    "$V(\\hat{\\mathbf{y}}) = \\sigma^2 \\cdot \\mathbf{H} \\cdot \\mathbf{H}$\n",
    "\n",
    "And since we proved in part (a) that $\\mathbf{H}$ is idempotent (i.e., $\\mathbf{H}\\mathbf{H} = \\mathbf{H}$):\n",
    "\n",
    "$V(\\hat{\\mathbf{y}}) = \\sigma^2 \\cdot \\mathbf{H}$\n",
    "\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Question 5. (20%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a simple linear regression model,  \n",
    "\n",
    "#### ✅ 5-a. show that the elements of the hat matrix can be expressed as:\n",
    "$h_{ii} = \\frac{1}{n} + \\frac{ ( x_{i} - \\bar{x} )^2}{ S_{xx} }$  \n",
    "$h_{ij} = \\frac{1}{n} + \\frac{ ( x_{i} - \\bar{x} )( x_{j} - \\bar{x} )}{ S_{xx}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hat Matrix Elements in Simple Linear Regression\n",
    "\n",
    "## Step 1: Define the linear regression model\n",
    "Our simple linear regression model is:\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n",
    "\n",
    "In matrix form, this is:\n",
    "$$\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon}$$\n",
    "\n",
    "where $\\mathbf{X}$ is an $n \\times 2$ matrix with a column of 1's and a column of $x$ values.\n",
    "\n",
    "## Step 2: Define the hat matrix\n",
    "The hat matrix $\\mathbf{H}$ maps the observed values $\\mathbf{y}$ to the fitted values $\\mathbf{\\hat{y}}$:\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{H} \\mathbf{y}$$\n",
    "\n",
    "The hat matrix is defined as:\n",
    "$$\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$$\n",
    "\n",
    "## Step 3: Expand the design matrix $\\mathbf{X}$\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## Step 4: Calculate $\\mathbf{X}^T\\mathbf{X}$\n",
    "$$\\mathbf{X}^T\\mathbf{X} = \\begin{bmatrix} \n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "x_1 & x_2 & \\cdots & x_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_n\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "n & \\sum_{i=1}^n x_i \\\\\n",
    "\\sum_{i=1}^n x_i & \\sum_{i=1}^n x_i^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## Step 5: Calculate $(\\mathbf{X}^T\\mathbf{X})^{-1}$\n",
    "Let's denote $\\sum_{i=1}^n x_i = n\\bar{x}$ and $S_{xx} = \\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n\\bar{x}^2$.\n",
    "\n",
    "The inverse of a $2 \\times 2$ matrix $\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$ is $\\frac{1}{ad-bc}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$.\n",
    "\n",
    "Applying this:\n",
    "$$(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{n\\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2}\n",
    "\\begin{bmatrix} \n",
    "\\sum_{i=1}^n x_i^2 & -\\sum_{i=1}^n x_i \\\\\n",
    "-\\sum_{i=1}^n x_i & n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Substituting our notations:\n",
    "$$(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{nS_{xx} + n^2\\bar{x}^2 - n^2\\bar{x}^2}\n",
    "\\begin{bmatrix} \n",
    "S_{xx} + n\\bar{x}^2 & -n\\bar{x} \\\\\n",
    "-n\\bar{x} & n\n",
    "\\end{bmatrix} = \n",
    "\\frac{1}{nS_{xx}}\n",
    "\\begin{bmatrix} \n",
    "S_{xx} + n\\bar{x}^2 & -n\\bar{x} \\\\\n",
    "-n\\bar{x} & n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "## Step 6: Calculate $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n",
    "Consider the $i$-th row of $\\mathbf{X}$, which is $[1 \\, x_i]$, and the $j$-th column of $\\mathbf{X}^T$, which is $[1 \\, x_j]^T$.\n",
    "\n",
    "The $(i,j)$ element of $\\mathbf{H}$ is:\n",
    "$$h_{ij} = [1 \\, x_i] \\cdot (\\mathbf{X}^T\\mathbf{X})^{-1} \\cdot [1 \\, x_j]^T$$\n",
    "\n",
    "Expanding this with the inverse we calculated:\n",
    "$$h_{ij} = [1 \\, x_i] \\cdot \\frac{1}{nS_{xx}}\n",
    "\\begin{bmatrix} \n",
    "S_{xx} + n\\bar{x}^2 & -n\\bar{x} \\\\\n",
    "-n\\bar{x} & n\n",
    "\\end{bmatrix} \\cdot [1 \\, x_j]^T$$\n",
    "\n",
    "$$h_{ij} = \\frac{1}{nS_{xx}} \\left[ (S_{xx} + n\\bar{x}^2) - n\\bar{x}x_i - n\\bar{x}x_j + nx_ix_j \\right]$$\n",
    "\n",
    "## Step 7: Simplify the expression for $h_{ij}$\n",
    "$$h_{ij} = \\frac{1}{nS_{xx}} \\left[ S_{xx} + n\\bar{x}^2 - n\\bar{x}x_i - n\\bar{x}x_j + nx_ix_j \\right]$$\n",
    "\n",
    "$$h_{ij} = \\frac{S_{xx}}{nS_{xx}} + \\frac{n\\bar{x}^2}{nS_{xx}} - \\frac{n\\bar{x}x_i}{nS_{xx}} - \\frac{n\\bar{x}x_j}{nS_{xx}} + \\frac{nx_ix_j}{nS_{xx}}$$\n",
    "\n",
    "$$h_{ij} = \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} - \\frac{\\bar{x}x_i}{S_{xx}} - \\frac{\\bar{x}x_j}{S_{xx}} + \\frac{x_ix_j}{S_{xx}}$$\n",
    "\n",
    "Rearranging terms:\n",
    "$$h_{ij} = \\frac{1}{n} + \\frac{1}{S_{xx}}(\\bar{x}^2 - \\bar{x}x_i - \\bar{x}x_j + x_ix_j)$$\n",
    "\n",
    "$$h_{ij} = \\frac{1}{n} + \\frac{1}{S_{xx}}(x_ix_j - \\bar{x}x_i - \\bar{x}x_j + \\bar{x}^2)$$\n",
    "\n",
    "This can be rewritten as:\n",
    "$$h_{ij} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})(x_j - \\bar{x})}{S_{xx}}$$\n",
    "\n",
    "## Step 8: Special case for $h_{ii}$ (diagonal elements)\n",
    "For the diagonal elements where $i = j$:\n",
    "$$h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})(x_i - \\bar{x})}{S_{xx}} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}$$\n",
    "\n",
    "## Summary\n",
    "Therefore, the elements of the hat matrix can be expressed as:\n",
    "- Diagonal elements: $h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}$\n",
    "- Off-diagonal elements: $h_{ij} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})(x_j - \\bar{x})}{S_{xx}}$\n",
    "\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ 5-b-1. Behavior of Diagonal Elements $h_{ii}$\n",
    "\n",
    "The diagonal element $h_{ii}$ is given by:\n",
    "$$h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{S_{xx}}$$\n",
    "\n",
    "Where:\n",
    "- $\\frac{1}{n}$ is a constant term\n",
    "- $\\frac{(x_i - \\bar{x})^2}{S_{xx}}$ depends on how far $x_i$ is from the mean $\\bar{x}$\n",
    "\n",
    "As $x_i$ deviates from $\\bar{x}$:\n",
    "1. When $x_i = \\bar{x}$:\n",
    "   - $(x_i - \\bar{x})^2 = 0$\n",
    "   - $h_{ii} = \\frac{1}{n}$ (minimum value)\n",
    "\n",
    "2. When $x_i$ moves away from $\\bar{x}$ in either direction:\n",
    "   - $(x_i - \\bar{x})^2$ increases quadratically\n",
    "   - $h_{ii}$ increases quadratically\n",
    "\n",
    "3. When $x_i$ is far from $\\bar{x}$:\n",
    "   - $h_{ii}$ becomes larger, approaching 1 for extreme values\n",
    "   - $h_{ii} \\to 1$ as $|x_i - \\bar{x}| \\to \\text{extreme}$\n",
    "\n",
    "This means observations with $x$ values far from the mean have higher \"leverage\" in the regression. These high-leverage points have greater influence on the fitted regression line, as their $h_{ii}$ values indicate how much the fitted value $\\hat{y}_i$ depends on the observed value $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ✅ 5-b-2. Behavior of Off-Diagonal Elements $h_{ij}$\n",
    "\n",
    "The off-diagonal element $h_{ij}$ is given by:\n",
    "$$h_{ij} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})(x_j - \\bar{x})}{S_{xx}}$$\n",
    "\n",
    "The behavior of $h_{ij}$ depends on both $x_i$ and $x_j$ relative to $\\bar{x}$:\n",
    "\n",
    "1. When either $x_i = \\bar{x}$ or $x_j = \\bar{x}$:\n",
    "   - $(x_i - \\bar{x})(x_j - \\bar{x}) = 0$\n",
    "   - $h_{ij} = \\frac{1}{n}$\n",
    "\n",
    "2. When $x_i$ and $x_j$ are on the same side of $\\bar{x}$ (both above or both below):\n",
    "   - $(x_i - \\bar{x})(x_j - \\bar{x}) > 0$ (positive)\n",
    "   - $h_{ij} > \\frac{1}{n}$ (larger than the baseline)\n",
    "\n",
    "3. When $x_i$ and $x_j$ are on opposite sides of $\\bar{x}$:\n",
    "   - $(x_i - \\bar{x})(x_j - \\bar{x}) < 0$ (negative)\n",
    "   - $h_{ij} < \\frac{1}{n}$ (smaller than the baseline)\n",
    "   - In extreme cases, $h_{ij}$ can become negative\n",
    "\n",
    "This indicates how observations with $x$ values influence each other's fitted values. Observations on the same side of the mean tend to positively influence each other's predictions, while observations on opposite sides of the mean can negatively influence each other's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications\n",
    "\n",
    "1. **Leverage points**: Observations with $x$ values far from $\\bar{x}$ have high $h_{ii}$ values and thus high leverage, making them potentially influential in determining the regression line.\n",
    "\n",
    "2. **Influence distribution**: As the leverage increases for observations far from the mean, the influence of observations near the mean decreases.\n",
    "\n",
    "3. **Outlier sensitivity**: The quadratic relationship means that extreme $x$ values have disproportionately high influence, making the regression sensitive to outliers in the predictor space.\n",
    "\n",
    "4. **Sum constraint**: The sum of all $h_{ii}$ equals 2 for simple linear regression, so if some observations gain leverage, others must lose it.\n",
    "\n",
    "5. **Model interpretation**: The hat matrix helps identify which observations are most influential in determining the fitted regression line, which is valuable for diagnostics and outlier detection.\n",
    "\n",
    "In practice, observations with $h_{ii} > \\frac{2p}{n}$ (where $p$ is the number of parameters, here $p=2$) are often considered high-leverage points that warrant special attention in regression diagnostics.\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Question 6. (20%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the true underlying model is\n",
    "$𝐲 = {\\beta}_0 + {\\beta}_1 * 𝐱_1 + {\\beta}_2 * 𝐱_2 + ⋯ {\\beta}_p * 𝐱_𝑝 + 𝛜 = 𝐗 * {\\beta} + 𝛜,$\n",
    "\n",
    "If we intentionally ignores the intercept term and fit the data with the following model:  \n",
    "$𝐲 = \\hat{\\beta}_1 * 𝐱_1 + \\hat{\\beta}_2 * 𝐱_2 + ⋯ \\hat{\\beta}_p * 𝐱_𝑝 = 𝐗_{prime} * \\hat{\\beta}_{prim}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ 6-a. (10%) Show that \n",
    "\n",
    "$E[\\hat{{\\beta}}_1] \\neq \\beta_1$\n",
    "$E[\\hat{{\\beta}}_2] \\neq \\beta_2$\n",
    "...\n",
    "$E[\\hat{{\\beta}}_p] \\neq \\beta_p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Model vs. Fitted Model\n",
    "\n",
    "- **True Model**: $\\mathbf{y} = \\beta_0 + \\beta_1\\mathbf{x}_1 + \\beta_2\\mathbf{x}_2 + \\cdots + \\beta_p\\mathbf{x}_p + \\boldsymbol{\\epsilon} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "  \n",
    "  Where $\\mathbf{X} = [\\mathbf{1}, \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p]$ and $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_p]^T$\n",
    "\n",
    "- **Fitted Model**: $\\mathbf{y} = \\hat{\\beta}_1\\mathbf{x}_1 + \\hat{\\beta}_2\\mathbf{x}_2 + \\cdots + \\hat{\\beta}_p\\mathbf{x}_p = \\mathbf{X}_{prime}\\hat{\\boldsymbol{\\beta}}_{prime}$\n",
    "  \n",
    "  Where $\\mathbf{X}_{prime} = [\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p]$ and $\\hat{\\boldsymbol{\\beta}}_{prime} = [\\hat{\\beta}_1, \\hat{\\beta}_2, \\ldots, \\hat{\\beta}_p]^T$\n",
    "\n",
    "## Parameter Estimation\n",
    "\n",
    "For the fitted model, the least squares estimate is:\n",
    "$$\\hat{\\boldsymbol{\\beta}}_{prime} = (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{y}$$\n",
    "\n",
    "Substituting the true model:\n",
    "$$\\hat{\\boldsymbol{\\beta}}_{prime} = (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon})$$\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}}_{prime} = (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\boldsymbol{\\epsilon}$$\n",
    "\n",
    "## Expected Value\n",
    "\n",
    "Taking the expected value:\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = E[(\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\boldsymbol{\\epsilon}]$$\n",
    "\n",
    "Since $E[\\boldsymbol{\\epsilon}] = \\mathbf{0}$:\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{X}\\boldsymbol{\\beta}$$\n",
    "\n",
    "Now, let's expand this. We have:\n",
    "$$\\mathbf{X} = [\\mathbf{1}, \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p]$$\n",
    "$$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_p]^T$$\n",
    "\n",
    "So:\n",
    "$$\\mathbf{X}\\boldsymbol{\\beta} = \\beta_0\\mathbf{1} + \\beta_1\\mathbf{x}_1 + \\beta_2\\mathbf{x}_2 + \\cdots + \\beta_p\\mathbf{x}_p$$\n",
    "\n",
    "Substituting back:\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T(\\beta_0\\mathbf{1} + \\beta_1\\mathbf{x}_1 + \\beta_2\\mathbf{x}_2 + \\cdots + \\beta_p\\mathbf{x}_p)$$\n",
    "\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\beta_0\\mathbf{1} + (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T(\\beta_1\\mathbf{x}_1 + \\beta_2\\mathbf{x}_2 + \\cdots + \\beta_p\\mathbf{x}_p)$$\n",
    "\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = \\beta_0(\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{1} + (\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{X}_{prime}[\\beta_1, \\beta_2, \\ldots, \\beta_p]^T$$\n",
    "\n",
    "Since $(\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{X}_{prime} = \\mathbf{I}$ (identity matrix):\n",
    "\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = \\beta_0(\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{1} + [\\beta_1, \\beta_2, \\ldots, \\beta_p]^T$$\n",
    "\n",
    "## The Key Point\n",
    "\n",
    "The critical term here is $\\beta_0(\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{1}$. Unless all predictors are centered (i.e., all $\\mathbf{x}_i$ have mean 0), this term will not be zero.\n",
    "\n",
    "Let's denote this bias term as $\\mathbf{b} = \\beta_0(\\mathbf{X}_{prime}^T\\mathbf{X}_{prime})^{-1}\\mathbf{X}_{prime}^T\\mathbf{1}$.\n",
    "\n",
    "Then:\n",
    "$$E[\\hat{\\boldsymbol{\\beta}}_{prime}] = \\mathbf{b} + [\\beta_1, \\beta_2, \\ldots, \\beta_p]^T$$\n",
    "\n",
    "For each individual coefficient:\n",
    "$$E[\\hat{\\beta}_1] = b_1 + \\beta_1 \\neq \\beta_1 \\text{ (unless } b_1 = 0\\text{)}$$\n",
    "$$E[\\hat{\\beta}_2] = b_2 + \\beta_2 \\neq \\beta_2 \\text{ (unless } b_2 = 0\\text{)}$$\n",
    "$$\\vdots$$\n",
    "$$E[\\hat{\\beta}_p] = b_p + \\beta_p \\neq \\beta_p \\text{ (unless } b_p = 0\\text{)}$$\n",
    "\n",
    "The bias terms $(b_1, b_2, \\ldots, b_p)$ will only be zero if:\n",
    "1. $\\beta_0 = 0$ (the true intercept is zero), or\n",
    "2. $\\mathbf{X}_{prime}^T\\mathbf{1} = \\mathbf{0}$ (all predictors are centered, i.e., have mean zero)\n",
    "\n",
    "Since we're not assuming either of these conditions, we have shown that ignoring the intercept term when it exists in the true model leads to biased estimates of all regression coefficients:\n",
    "\n",
    "$$E[\\hat{\\beta}_j] \\neq \\beta_j \\text{ for } j = 1, 2, \\ldots, p$$\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ 6-b. (5%) What can you conclude from (a)?\n",
    "All estimates are **BIASED** when we omit the intercept term from a model where it truly exists.\n",
    "## 👍 DONE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Question 7. (15%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multiple regression model: $𝐲 = 𝐗 * \\beta + 𝛜$, it is critical to know if $((𝐗^𝑇)*𝐗)^{−1}$ exists.  \n",
    "The diagonal elements of $((𝐗^𝑇)*𝐗)^{−1}$ in the correlation form, where 𝐗 is standardized, are known as Variance Inflation Factors\n",
    "(VIFs). They are crucial to diagnose multicollinearity. VIF for the 𝑗-𝑡ℎ regression coefficient is expressed as\n",
    "\n",
    "${VIF}_{j} = \\frac{1}{1-(R_j)^2}$\n",
    "\n",
    "$R_j$ is the coefficient of multiple determination obtained from regressing 𝐱_𝑗 on the other regressor variables (𝐱_1 to 𝐱_𝑝, except 𝐱_𝑗). Calculate all the VIFs in the “autompg” dataset and discuss your observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cylinders</td>\n",
       "      <td>10.737535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>displacement</td>\n",
       "      <td>21.836792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horsepower</td>\n",
       "      <td>9.943693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weight</td>\n",
       "      <td>10.831260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acceleration</td>\n",
       "      <td>2.625806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model_year</td>\n",
       "      <td>1.244952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>origin</td>\n",
       "      <td>1.772386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Variable        VIF\n",
       "0     cylinders  10.737535\n",
       "1  displacement  21.836792\n",
       "2    horsepower   9.943693\n",
       "3        weight  10.831260\n",
       "4  acceleration   2.625806\n",
       "5    model_year   1.244952\n",
       "6        origin   1.772386"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 必要なライブラリを再ロード\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# ファイルパス\n",
    "filePath = \"./auto-mpg.data.txt\"\n",
    "\n",
    "# データセットの説明に基づく列名\n",
    "columnNames = [\n",
    "    \"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \n",
    "    \"acceleration\", \"model_year\", \"origin\", \"car_name\"\n",
    "]\n",
    "\n",
    "# データセットをロード\n",
    "df = pd.read_csv(filePath, delim_whitespace=True, names=columnNames, na_values='?')\n",
    "\n",
    "# 'horsepower' の欠損値を持つ行を削除\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 'horsepower' を float に変換（欠損値のために文字列として読み込まれた）\n",
    "df['horsepower'] = df['horsepower'].astype(float)\n",
    "\n",
    "# 'car_name' は数値ではないため削除\n",
    "df.drop(columns=['car_name'], inplace=True)\n",
    "\n",
    "# 独立変数（'mpg' は従属変数なので除外）を定義\n",
    "X = df.drop(columns=['mpg'])\n",
    "\n",
    "# 独立変数を標準化（相関形式）\n",
    "scaler = StandardScaler()\n",
    "xScaled = scaler.fit_transform(X)\n",
    "\n",
    "# 各独立変数の VIF を計算\n",
    "vifs = pd.DataFrame()\n",
    "vifs[\"Variable\"] = X.columns\n",
    "vifs[\"VIF\"] = [variance_inflation_factor(xScaled, i) for i in range(xScaled.shape[1])]\n",
    "\n",
    "# 計算された VIF を表示\n",
    "vifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\text{Variable} & \\text{VIF} \\\\\n",
    "\\hline\n",
    "\\text{cylinders} & 10.74 \\\\\n",
    "\\text{displacement} & 21.84 \\\\\n",
    "\\text{horsepower} & 9.94 \\\\\n",
    "\\text{weight} & 10.83 \\\\\n",
    "\\text{acceleration} & 2.63 \\\\\n",
    "\\text{model\\_year} & 1.24 \\\\\n",
    "\\text{origin} & 1.77 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VIF分析（auto-mpgデータセット）からの結論：**\n",
    "\n",
    "* **高い多重共線性：**\n",
    "    * \"cylinders\"、\"displacement\"、\"horsepower\"、\"weight\"は高いVIF値（5または10より大きい）を示す。\n",
    "    * これらの変数間に強い線形関係が存在する。\n",
    "* **中程度の多重共線性：**\n",
    "    * \"acceleration\"は中程度の多重共線性を示す。\n",
    "* **低い多重共線性：**\n",
    "    * \"model_year\"と\"origin\"は低い多重共線性を示す。\n",
    "* **影響：**\n",
    "    * 高い相関を持つ変数の係数推定値は信頼性が低い。\n",
    "    * 個々の変数の影響を評価することが困難。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Observations:**\n",
    "1. **High Multicollinearity:**  \n",
    "   - `displacement` (VIF ≈ 21.84), `weight` (VIF ≈ 10.83), and `cylinders` (VIF ≈ 10.74) have high VIF values, indicating strong multicollinearity.  \n",
    "   - This suggests that these variables are highly correlated with each other. Likely, `displacement`, `cylinders`, and `weight` are strongly related since larger engines tend to have more cylinders and weigh more.\n",
    "\n",
    "2. **Moderate Multicollinearity:**  \n",
    "   - `horsepower` (VIF ≈ 9.94) also exhibits some collinearity, which might be due to its correlation with engine displacement and weight.\n",
    "\n",
    "3. **Low Multicollinearity:**  \n",
    "   - `acceleration` (VIF ≈ 2.63) shows some correlation but is not extreme.\n",
    "   - `model_year` (VIF ≈ 1.24) and `origin` (VIF ≈ 1.77) have low VIF values, meaning they are relatively independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion:**\n",
    "- High VIF values suggest that some predictors (`displacement`, `cylinders`, `weight`, and `horsepower`) may need to be removed or transformed to reduce multicollinearity.\n",
    "- One approach to handling this would be **Principal Component Analysis (PCA)** or **removing redundant variables** to improve model interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
