{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM (expectation‚Äìmaximization) algorithm #\n",
    " - When modeling documents with multivariate Bernoulli distributions, we represent document d as a binary vector indicating whether a word occurs or not in $d_k$.  \n",
    " - More specifically, given vocabulary $W = {w_1, ..., w_M}$ with M words, document $d_k$ is represented as $d_k = (x_{1,k}, x_{2,k}, ... x_{M,k})$, where $x_{i,k}$ is either 0 or 1. \n",
    " - $x_i =1$ if word $w_i$ can be observed in $d_k$; otherwise, $x_{i,k} =0$.   \n",
    " - Assume that there are totally N documents in corpus $C = {d_i, ..., d_N}$, i.e., $k=1..N$.  \n",
    " - We want to model the N documents with a mixture model with two multivariate Bernoulli distributions $Œ∏_1$ and $Œ∏_2$. \n",
    " - Each component $Œ∏_j (j=1,2)$ has M parameters ${ p(w_i = 1 | Œ∏_j ) } (i=1,2,...M)$, where $p(w_i = 1 | Œ∏_j )$ means the probability that w_i would show up when using Œ∏_j to generate a document. \n",
    " - Similarly, $p(w_i = 0 | Œ∏_j )$ means the probability that w_i would not show up when using Œ∏_j to generate a document. \n",
    " - $p(w_i = 0 | Œ∏_j ) + p(w_i = 1 | Œ∏_j ) = 1$.  \n",
    " - Suppose we choose Œ∏_1 with probability $\\lambda_1$, and Œ∏_2 with probability $\\lambda_2$.  \n",
    "$\\lambda_1 + \\lambda_2 = 1$  \n",
    "\n",
    "(a) Please define the log-likelihood function for $p( d_k | Œ∏_1 + Œ∏_2 )$ given such a two-component mixture model.\n",
    "(b) Suppose we know $p( w_i =1 | Œ∏_1 )$ and $p( w_i =1 | Œ∏_2 )$. Write down the E-step and M-step formulas for estimating $\\lambda_1$ and $\\lambda_2$.  Show your calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question (a)\n",
    "Please define the log-likelihood function for $p( d_k | Œ∏_1 + Œ∏_2 )$ given such a two-component mixture model. \n",
    "\n",
    "## üó£Ô∏è Response (a)\n",
    "1. From page 04 of of the lecture slide [em_2025.pdf](https://www.csie.ntu.edu.tw/~pjcheng/course/wm2025/em_2025.pdf), we know that, given a two-component mixture model, the probability of observing document $d_k$ is $$ p(d_k | \\theta_1 + \\theta_2) = \\lambda_1 p(d_k | \\theta_1) + \\lambda_2 p(d_k | \\theta_2) $$\n",
    "\n",
    "2. For a multivariate [Bernoulli](https://en.wikipedia.org/wiki/Bernoulli_distribution) distribution, the probability of document $ d_k $ given component $ \\theta_j $ is $$ p(d_k | \\theta_j) = \\prod_{i=1}^{M} p(w_i = 1 | \\theta_j)^{x_{i,k}} p(w_i = 0 | \\theta_j)^{(1-x_{i,k})} = \\prod_{i=1}^{M} p(w_i = 1 \\mid \\theta_j)^{x_{i,k}} \\cdot (1 - p(w_{i} = 1 \\mid \\theta_j))^{1 - x_{i,k}} $$  \n",
    "- $ x_{i,k} = 1 $ if word $ w_{i} $ appears in document $ d_{k} $, and 0 otherwise\n",
    "- $ p(w_i = 0 \\mid \\theta_j) = 1 - p(w_i = 1 \\mid \\theta_j) $\n",
    "```\n",
    "Comment by TA CHIU, TZU-WEI concerning the annoatation of $x_{i,k}$\n",
    "You can add it or leave it out ‚Äî as long as the meaning is clearly conveyed, it's fine either way. No points will be deducted if you don't include the additional subscript.\n",
    "```\n",
    "\n",
    "3. The log-likelihood function for a single document $ d_k $ is: $\\log p(d_k | \\theta_1 + \\theta_2) = \\log\\left[ \\lambda_1 p(d_k | \\theta_1) + \\lambda_2 p(d_k | \\theta_2) \\right] = \\log \\Big[ \\lambda_1 \\prod_{i=1}^{M} p(w_i = 1 | \\theta_1)^{x_{i,k}} p(w_i = 0 | \\theta_1)^{(1-x_{i,k})} + \\lambda_2 \\prod_{i=1}^{M} p(w_i = 1 | \\theta_2)^{x_{i,k}} p(w_i = 0 | \\theta_2)^{(1-x_{i,k})} \\Big]$\n",
    "\n",
    "4. The log-likelihood function for the entire corpus $C$ is (just sum the log likelihood over all the documents in the corpus.): $\\log p(C \\mid \\theta_1, \\theta_2, \\lambda_1, \\lambda_2) = \\sum_{k=1}^{N} p(d_k | \\theta_1 + \\theta_2) = \\sum_{k=1}^{N} \\log\\left[ \\lambda_1 \\cdot p(d_k \\mid \\theta_1) + \\lambda_2 \\cdot p(d_k \\mid \\theta_2) \\right] = \\sum_{k=1}^{N} \\log \\Big[ \\lambda_1 \\prod_{i=1}^{M} p(w_i = 1 | \\theta_1)^{x_{i,k}} p(w_i = 0 | \\theta_1)^{(1-x_{i,k})} + \\lambda_2 \\prod_{i=1}^{M} p(w_i = 1 | \\theta_2)^{x_{i,k}} p(w_i = 0 | \\theta_2)^{(1-x_{i,k})} \\Big] $\n",
    "\n",
    "```\n",
    "Comment by TA CHIU, TZU-WEI concerning the form we need to reduce to.\n",
    "In principle, you should simplify the expression as much as possible. So if you believe it cannot be further simplified meaningfully, then it's acceptable to leave it in that form.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Question (b) Expectation-Maximization (EM) Steps\n",
    "\n",
    "## üó£Ô∏è Response (b)\n",
    "\n",
    "#### **E-step: Compute Responsibilities (Expectation) (page 22 of [em_2025.pdf](https://www.csie.ntu.edu.tw/~pjcheng/course/wm2025/em_2025.pdf))**\n",
    "\n",
    " - Define $ Œ≥_{k,1} $ as the posterior probabilities (responsibilities) of document $ d_k $ belonging to component $ \\theta_1 $\n",
    " - Define $ Œ≥_{k,2} $ as the posterior probabilities (responsibilities) of document $ d_k $ belonging to component $ \\theta_2 $\n",
    "\n",
    "$$Œ≥_{k,1} = \\frac{\\lambda_1 p(d_k | \\theta_1)}{\\lambda_1 p(d_k | \\theta_1) + \\lambda_2 p(d_k | \\theta_2)}$$\n",
    "$$Œ≥_{k,2} = \\frac{\\lambda_2 p(d_k | \\theta_2)}{\\lambda_1 p(d_k | \\theta_1) + \\lambda_2 p(d_k | \\theta_2)}$$\n",
    "\n",
    "where from problem (a), we know that \n",
    "$$ p(d_k | \\theta_j) = \\prod_{i=1}^{M} p(w_i = 1 | \\theta_j)^{x_{i,k}} p(w_i = 0 | \\theta_j)^{(1-x_{i,k})} = \\prod_{i=1}^{M} p(w_i = 1 \\mid \\theta_j)^{x_{i,k}} \\cdot (1 - p(w_{i} = 1 \\mid \\theta_j))^{1 - x_{i,k}} $$  \n",
    "\n",
    "\n",
    "To examine this probability, we may adopt the idea of incomplete data, we can define $ z_k $ is the latent (hidden) variable indicating which component generated document $ d_k $. To be more specific, a hidden variable $z_k$ for each document $d_k$, where:\n",
    "- $z_k = 1$ if document $d_k$ was generated from component Œ∏‚ÇÅ\n",
    "- $z_k = 2$ if document $d_k$ was generated from component Œ∏‚ÇÇ\n",
    "\n",
    "Following this definition, the posterior probability that document $ d_k $ was generated by component $ j $ (given the current parameter estimates.) can be expressed by:\n",
    "$$p(z_k=j|d_k,\\theta_1,\\theta_2,\\lambda_1,\\lambda_2) = \\frac{\\lambda_j \\cdot p(d_k|\\theta_j)}{\\lambda_1 \\cdot p(d_k|\\theta_1) + \\lambda_2 \\cdot p(d_k|\\theta_2)}$$\n",
    "\n",
    "\n",
    "- For $ j = 1 $:$Œ≥_{k,1} =\\gamma(z_k = 1) = \\frac{\\lambda_1 \\cdot p(d_k \\mid \\theta_1)}{\\lambda_1 \\cdot p(d_k \\mid \\theta_1) + \\lambda_2 \\cdot p(d_k \\mid \\theta_2)}$\n",
    "- For $ j = 2 $:$Œ≥_{k,2} =\\gamma(z_k = 2) = \\frac{\\lambda_2 \\cdot p(d_k \\mid \\theta_2)}{\\lambda_1 \\cdot p(d_k \\mid \\theta_1) + \\lambda_2 \\cdot p(d_k \\mid \\theta_2)}$\n",
    "\n",
    "To TA, the following is the simplest form I can get.\n",
    "\n",
    "$$\\gamma(z_{k}=j) = \\frac{\\lambda_j \\cdot \\prod_{i=1}^M [p(w_i=1|\\theta_j)]^{x_{i,k}} \\cdot [1-p(w_i=1|\\theta_j)]^{1-x_{i,k}}}{\\sum_{l=1}^2 \\lambda_l \\cdot \\prod_{i=1}^M [p(w_i=1|\\theta_l)]^{x_{i,k}} \\cdot [1-p(w_i=1|\\theta_l)]^{1-x_{i,k}}}$$\n",
    "\n",
    "#### **M-step: Update Parameters**\n",
    "In the M-step, we update the model parameters to maximize the expected log-likelihood. Given that we already know $p(w_i=1|\\theta_1)$ and $p(w_i=1|\\theta_2)$ (as stated in the problem), we only need to updates the mixture weights $ \\lambda_1 $ and $ \\lambda_2 $ to maximize the expected log-likelihood.\n",
    "\n",
    "The updated estimate for $\\lambda_j$ is the average posterior probability of component $j$ across all documents:\n",
    "\n",
    "$$\\lambda_j^{new} = \\frac{1}{N} \\sum_{k=1}^N \\gamma(z_{k}=j)$$\n",
    "\n",
    "$\n",
    "Œª_1^{new} = \\frac{1}{N} \\sum_{k=1}^{N} Œ≥_{k,1} = \\frac{1}{N} \\sum_{k=1}^{N} \\gamma(z_k = 1)\n",
    "$\n",
    "\n",
    "$\n",
    "Œª_2^{new} = \\frac{1}{N} \\sum_{k=1}^{N} Œ≥_{k,2} = \\frac{1}{N} \\sum_{k=1}^{N} \\gamma(z_k = 2)\n",
    "$\n",
    "\n",
    "1. Since $\\gamma(z_{k,1}) + \\gamma(z_{k,2}) = 1$ for each document $d_k$. This ensures that $ Œª_1 + Œª_2 = 1 $, maintaining the constraint that the mixture weights must sum to 1.\n",
    "2. **Calculation Steps:** \n",
    "    1. For each document $ d_k $, compute $ \\gamma(z_k = 1) $ and $ \\gamma(z_k = 2) $ using the E-step formulas.\n",
    "    2. Sum these posterior probabilities across all documents.\n",
    "    3. Divide by $ N $ to obtain the updated mixture weights.\n",
    "3. The calculation steps above computes the expected number of documents assigned to each component and normalizing by the total number of documents.\n",
    "4. Note that we do not update the word probabilities $p(w_i = 1 | \\theta_j)$ since they are given as KNWON values as described in the problem description of part (b).\n",
    "\n",
    "---\n",
    "\n",
    "### EM Iteration Pseudo Code (page 25 of [em_2025.pdf](https://www.csie.ntu.edu.tw/~pjcheng/course/wm2025/em_2025.pdf))\n",
    "1. **Initialize** $\\lambda_1$ and $\\lambda_2$ (ensuring $\\lambda_1 + \\lambda_2 = 1$)\n",
    "2. **E-step:** Compute responsibilities $\\gamma_{k,1}$ and $\\gamma_{k,2}$ for each document\n",
    "3. **M-step:** Update the mixture coefficients $\\lambda_1$ and $\\lambda_2$\n",
    "4. **Repeat** steps 2-3 until convergence"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
