# -*- coding: utf-8 -*-
"""notebookbf0f1e7a24

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kenkaocwl/notebookbf0f1e7a24.b26ba246-c43d-43e3-87ec-ff968bc78d31.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251001/auto/storage/goog4_request%26X-Goog-Date%3D20251001T110131Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6ef89d6e148b3f5e0415034537b91fe068a00804d9530c260e5631328c6d4fcb6c6df11996f825fcbbd238d23ee7f7e59561b1f5551e1eb5c3ae376a4be31fefef9225cb049c353c0ddd94f788ae528795e217dd35d7c265ec01f9c0a52796907e1df7c1c1d2d9b203cb4adb8a42b09107e2f7a5cb8977e0feeb453e71a3c71a155c6ae7ab607a9161ae5234ca93c673d9ea4515032bfedbf9f795d6e84ed04d3a06d402e356675f5674c69f845f4983ee9e4336db7634c35f4e711583f2878b83129c2e39074fbc7efcd650028e10d1a26a31b7f4dffada3b50e1d2bcf45896b8a668b371ee4da9404f6dadf7a21037e50af23166165ad08ccfded12f4caaa6
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

kenkaocwl_extractive_question_answering_path = kagglehub.dataset_download('kenkaocwl/extractive-question-answering')

print('Data source import complete.')

# Chinese Extractive Question Answering - Kaggle Notebook
# This notebook implements a two-stage approach: paragraph selection + span selection

# =============================================================================
# CELL 1: Setup and Imports
# =============================================================================

import json
import os
import csv
import math
import random
import numpy as np
import pandas as pd
from pathlib import Path
from itertools import chain
from tqdm.auto import tqdm

import torch
from torch.utils.data import DataLoader

# Transformers and related
import transformers
from transformers import (
    AutoConfig, AutoTokenizer,
    AutoModelForMultipleChoice, AutoModelForQuestionAnswering,
    DataCollatorForMultipleChoice, DataCollatorWithPadding,
    get_scheduler, SchedulerType
)

# Datasets and evaluation
import datasets
from datasets import Dataset, load_dataset

# Accelerate for distributed training
from accelerate import Accelerator
from accelerate.utils import set_seed

# Set up logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("All imports successful!")
print(f"PyTorch version: {torch.__version__}")
print(f"Transformers version: {transformers.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# =============================================================================
# CELL 2: Data Loading and Preprocessing Functions
# =============================================================================

def load_data():
    """Load all dataset files from Kaggle input"""
    # In Kaggle, your uploaded datasets will be in /kaggle/input/your-dataset-name/
    # Adjust the path based on your dataset name
    data_path = "/kaggle/input"  # Update this path to your dataset

    # Find the correct dataset folder
    dataset_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]
    print(f"Available datasets: {dataset_folders}")

    # Use the first dataset folder or specify manually
    if dataset_folders:
        data_path = os.path.join(data_path, dataset_folders[0])

    with open(os.path.join(data_path, 'context.json'), 'r', encoding='utf-8') as f:
        contexts = json.load(f)

    with open(os.path.join(data_path, 'train.json'), 'r', encoding='utf-8') as f:
        train_data = json.load(f)

    with open(os.path.join(data_path, 'valid.json'), 'r', encoding='utf-8') as f:
        valid_data = json.load(f)

    with open(os.path.join(data_path, 'test.json'), 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    return contexts, train_data, valid_data, test_data

def prepare_paragraph_selection_data(contexts, data):
    """Convert data to SWAG-like format for paragraph selection"""
    converted_data = []

    for item in data:
        question = item['question']
        paragraph_ids = item['paragraphs']
        paragraphs = [contexts[pid] for pid in paragraph_ids]

        converted_item = {
            'sent1': question,
            'sent2': '',
            'ending0': paragraphs[0],
            'ending1': paragraphs[1],
            'ending2': paragraphs[2],
            'ending3': paragraphs[3],
            'id': item['id']
        }

        if 'relevant' in item:
            relevant_id = item['relevant']
            label = paragraph_ids.index(relevant_id)
            converted_item['label'] = label

        converted_data.append(converted_item)

    return converted_data

def prepare_qa_data(contexts, data):
    """Convert data to SQuAD-like format for extractive QA"""
    converted_data = []

    for item in data:
        question = item['question']

        if 'relevant' in item:
            context = contexts[item['relevant']]
            answer_text = item['answer']['text']
            answer_start = item['answer']['start']

            converted_item = {
                'id': item['id'],
                'question': question,
                'context': context,
                'answers': {
                    'text': [answer_text],
                    'answer_start': [answer_start]
                }
            }
        else:
            converted_item = {
                'id': item['id'],
                'question': question,
                'context': '',
                'answers': {
                    'text': [],
                    'answer_start': []
                }
            }

        converted_data.append(converted_item)

    return converted_data

print("Data preprocessing functions defined!")

# =============================================================================
# CELL 3: Load and Preprocess Data
# =============================================================================

# Load data
contexts, train_data, valid_data, test_data = load_data()

print(f"Loaded {len(contexts)} contexts")
print(f"Loaded {len(train_data)} training examples")
print(f"Loaded {len(valid_data)} validation examples")
print(f"Loaded {len(test_data)} test examples")

# Create processed data
para_train_data = prepare_paragraph_selection_data(contexts, train_data)
para_valid_data = prepare_paragraph_selection_data(contexts, valid_data)
para_test_data = prepare_paragraph_selection_data(contexts, test_data)

qa_train_data = prepare_qa_data(contexts, train_data)
qa_valid_data = prepare_qa_data(contexts, valid_data)

print("Data preprocessing completed!")

# =============================================================================
# CELL 4: Paragraph Selection Model Training Functions
# =============================================================================

def preprocess_paragraph_function(examples, tokenizer, max_seq_length=512):
    """Preprocess examples for multiple choice"""
    first_sentences = [[context] * 4 for context in examples["sent1"]]
    question_headers = examples["sent2"]
    second_sentences = [
        [f"{header} {examples[end][i]}" for end in ["ending0", "ending1", "ending2", "ending3"]]
        for i, header in enumerate(question_headers)
    ]

    # Flatten
    first_sentences = list(chain(*first_sentences))
    second_sentences = list(chain(*second_sentences))

    # Tokenize
    tokenized_examples = tokenizer(
        first_sentences,
        second_sentences,
        max_length=max_seq_length,
        padding=True,
        truncation=True,
    )

    # Un-flatten
    tokenized_inputs = {k: [v[i:i+4] for i in range(0, len(v), 4)]
                       for k, v in tokenized_examples.items()}

    if "label" in examples:
        tokenized_inputs["labels"] = examples["label"]

    return tokenized_inputs

def train_paragraph_model():
    """Train the paragraph selection model"""
    print("Starting paragraph selection training...")

    # Hyperparameters
    model_name = "bert-base-chinese"
    max_seq_length = 512
    per_device_train_batch_size = 1
    gradient_accumulation_steps = 2
    learning_rate = 3e-5
    num_train_epochs = 1

    # Initialize accelerator
    accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)

    # Set seed
    set_seed(42)

    # Load model and tokenizer
    config = AutoConfig.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForMultipleChoice.from_pretrained(model_name, config=config)

    # Preprocess datasets
    train_dataset = Dataset.from_list(para_train_data)
    valid_dataset = Dataset.from_list(para_valid_data)

    def preprocess_with_tokenizer(examples):
        return preprocess_paragraph_function(examples, tokenizer, max_seq_length)

    train_dataset = train_dataset.map(
        preprocess_with_tokenizer,
        batched=True,
        remove_columns=train_dataset.column_names,
    )

    valid_dataset = valid_dataset.map(
        preprocess_with_tokenizer,
        batched=True,
        remove_columns=valid_dataset.column_names,
    )

    # Data collator and loaders
    data_collator = DataCollatorForMultipleChoice(tokenizer, return_tensors="pt")

    train_dataloader = DataLoader(
        train_dataset,
        shuffle=True,
        collate_fn=data_collator,
        batch_size=per_device_train_batch_size
    )

    eval_dataloader = DataLoader(
        valid_dataset,
        collate_fn=data_collator,
        batch_size=8
    )

    # Optimizer and scheduler
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
        {
            "params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate)

    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
    max_train_steps = num_train_epochs * num_update_steps_per_epoch

    lr_scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=max_train_steps,
    )

    # Prepare with accelerator
    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
    )

    # Training loop
    progress_bar = tqdm(range(max_train_steps))
    completed_steps = 0

    for epoch in range(num_train_epochs):
        model.train()
        total_loss = 0

        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(model):
                outputs = model(**batch)
                loss = outputs.loss
                total_loss += loss.detach().float()
                accelerator.backward(loss)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            if accelerator.sync_gradients:
                progress_bar.update(1)
                completed_steps += 1

            if completed_steps >= max_train_steps:
                break

        # Evaluation
        model.eval()
        correct = 0
        total = 0

        for batch in eval_dataloader:
            with torch.no_grad():
                outputs = model(**batch)

            predictions = outputs.logits.argmax(dim=-1)
            predictions, references = accelerator.gather_for_metrics((predictions, batch["labels"]))

            correct += (predictions == references).sum().item()
            total += len(references)

        accuracy = correct / total
        print(f"Epoch {epoch}: Accuracy = {accuracy:.4f}")

    # Save model
    output_dir = "paragraph_model"
    os.makedirs(output_dir, exist_ok=True)

    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, is_main_process=accelerator.is_main_process,
                                   save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)

    print("Paragraph selection model training completed!")
    return model, tokenizer

print("Paragraph selection functions defined!")

# =============================================================================
# CELL 5: Train Paragraph Selection Model
# =============================================================================

paragraph_model, paragraph_tokenizer = train_paragraph_model()